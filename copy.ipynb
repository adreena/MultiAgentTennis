{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "Implement multi-agent DDPG for the tennis environment using the Unity ML-Agents toolkit. \n",
    "\n",
    "The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model\n",
    "- I have captured portion of the runs in the files `Before_RL-01.m4v`, `After_RL_Slow-01.m4v` and `After_RL_Fast-01.m4v`. So one can run the mp4 file to see how the agent behaves.\n",
    "\n",
    "So one can either :\n",
    "- Run the cells in Part 1 and then Part 2 -> to train a model, explore hyperparameters and so forth (slow)\n",
    "- `Or` \n",
    "- Run cells in Part 1 and then Part 3 -> to run a stored model (fast)\n",
    "- `Or`\n",
    "- Watch the three videos viz. `Before_RL-01.m4v`, `After_RL_Slow-01.m4v` and `After_RL_Fast-01.m4v` (fastest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "The required setup is detailed in the README.md\n",
    "\n",
    "I am running this on a MacBookPro 14,3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 64 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 5e-4 # 1e-3 # 1e-4 # 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 6e-4 # 3e-3 # 3e-4  # 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [32,32] # [16,16] # [8,8] # [64,64] # [32,16] # [128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [32,32] # [16,16] # [8,8] # [64,64] # [32,16] # [128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = True # False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "`Note : The file_name might be different for different OS. As mentioned earlier, I am running OSX on a MAcBookPro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='Tennis_Linux/Tennis.x86_64') #'Reacher_20.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "- The cell below tests to make sure the environment is up and running by printing some information about the environment.\n",
    "- It also acquires the dimensions of the state and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "`Note : The file_name might be different for different OS. As mentioned earlier, I am running OSX on a MAcBookPro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Define classes and setup\n",
    "\n",
    "The device declaration enables the program leverage GPUs if they are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Learning Algorithm\n",
    "\n",
    "We are using the multi-agent DDPG as explained in the report.\n",
    "<img src=\"RL_Systems_Flow.png\" style=\"float:left\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks. The accompanying Report.pdf has more details on the agent and how multi-agent paradigms are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "# Finally used Xavier Initialization, which is very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): #True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate the shared agent instance\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=34, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----\n",
    "#### _**Note : If you want to run a stored model, skip Part 2 and run the cells in Part 3 below after the run logs & test area**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Multi-agent DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Manage the observations and actions for the two agents\n",
    "2. Display a running commentry of the scores, percentile statistics and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 0.5 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_ddpg(n_episodes=2000, max_t=700):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    steps_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    solved = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                 # get the current state (for each agent)\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        for t in range(max_t):\n",
    "            # print(state)\n",
    "            actions=[]\n",
    "            for a in range(num_agents):\n",
    "                action = agent.act(states[a])\n",
    "                actions.append(action)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            for a in range(num_agents):\n",
    "                agent.step(states[a], actions[a], rewards[a], next_states[a], dones[a])\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            score += np.max(rewards)\n",
    "            if any(dones):\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        steps_window.append(max_steps)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.3f}\\tMax_steps : {:4d}\\tSteps Avg : {:6.3f}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, np.max(steps_window),np.mean(steps_window)), end=\"\")\n",
    "        if i_episode % 500 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore Stats: {}\\tSteps Stats : {}'.\\\n",
    "                  format(i_episode, np.mean(scores_window), np.percentile(scores_window,[25,50,75,100]),\\\n",
    "                         np.percentile(steps_window,[25,50,75,100])))  \n",
    "        if (np.mean(scores_window) >= 0.5) and (not solved):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            solved = True\n",
    "            # break\n",
    "            # To see how far it can go\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the Multi-Agent DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\tAverage Score: 0.00\tScore Stats: [0. 0. 0. 0.]\tSteps Stats : [13. 13. 13. 14.]\n",
      "Episode 1000\tAverage Score: 0.00\tScore Stats: [0. 0. 0. 0.]\tSteps Stats : [13. 13. 13. 14.]\n",
      "Episode 1500\tAverage Score: 0.00\tScore Stats: [0.  0.  0.  0.1]\tSteps Stats : [13. 14. 14. 30.]\n",
      "Episode 2000\tAverage Score: 0.01\tScore Stats: [0.  0.  0.  0.1]\tSteps Stats : [14. 14. 15. 39.]\n",
      "Episode 2357\tAverage Score: 0.02\tScore: 0.000\tMax_steps :   47\tSteps Avg : 17.430"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ma_ddpg(n_episodes=10000,max_t=1000) # 2000,1500 or 1500,1500 ; quick test 100,500\n",
    "# The env ends at 1000 steps. Tried max_t > 1K. Didn't see any complex adaptive temporal behavior\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Logs\n",
    "[10/20/18] Initial try\n",
    "100 episode - nothing happens\n",
    "Trying 100000\n",
    "FC32-FC16-FC4\n",
    "** [10/20/18 6:40 PM] FC64-FC64-FC2\n",
    "worse - nothing more than 0. Rackets collide !\n",
    "** FC8-FC8-FC2\n",
    "Episode 3300\tAverage Score: 0.07\n",
    "Episode 3400\tAverage Score: 0.06\n",
    "Episode 3500\tAverage Score: 0.08\n",
    "Episode 3600\tAverage Score: 0.06\n",
    "Episode 3700\tAverage Score: 0.06\n",
    "Episode 3800\tAverage Score: 0.05\n",
    "Episode 3900\tAverage Score: 0.05\n",
    "Episode 3961\tAverage Score: 0.02\n",
    "not good\n",
    "** lr to 1e-4\n",
    "no learning after ~1000 that too was at 0.01 et al.\n",
    "** lr to 0.001 - not learning anythong. Rackets hit each other\n",
    "** FC16-FC16-FC2 lr 5e-4 3e-4\n",
    "Getting better by ~4500. even went upto 500 steps !\n",
    "Solved in 5400 steps. if max_t was higher it would have gone more !\n",
    "But goes down after that !\n",
    "2.6 at steps 500\n",
    "0.5 ~ 190 steps\n",
    "stop at 500 because don't want to spike, may be later.\n",
    "Episode 6300\tAverage Score: 0.39\n",
    "Episode 6400\tAverage Score: 0.56\n",
    "Episode 6500\tAverage Score: 0.58\n",
    "Episode 6600\tAverage Score: 0.44\n",
    "Episode 6700\tAverage Score: 0.76\n",
    "Episode 6800\tAverage Score: 1.04\n",
    "Episode 6900\tAverage Score: 0.37\n",
    "Episode 7000\tAverage Score: 1.33\n",
    "Episode 7095\tAverage Score: 0.25\tScore: 1.2000000178813934\tMax_steps : 228\n",
    "Not consistent at ~7000, > 100000 episodes ?\n",
    "Episode 7900\tAverage Score: 1.45\n",
    "Episode 7908\tAverage Score: 1.50\tScore: 2.500000037252903\tMax_steps : 500\n",
    "Larger Network ?\n",
    "Episode 8400\tAverage Score: 0.97\n",
    "Episode 8500\tAverage Score: 1.09\n",
    "Episode 8600\tAverage Score: 0.31\n",
    "Episode 8700\tAverage Score: 0.25\n",
    "Episode 8800\tAverage Score: 0.43\n",
    "Episode 8900\tAverage Score: 0.28\n",
    "Episode 9000\tAverage Score: 0.34\n",
    "Episode 9100\tAverage Score: 0.26\n",
    "Episode 9200\tAverage Score: 0.33\n",
    "Episode 9300\tAverage Score: 0.86\n",
    "Episode 9400\tAverage Score: 0.43\n",
    "Episode 9500\tAverage Score: 0.18\n",
    "Episode 9600\tAverage Score: 0.23\n",
    "Episode 9700\tAverage Score: 0.19\n",
    "Episode 9800\tAverage Score: 0.21\n",
    "Episode 9900\tAverage Score: 0.26\n",
    "Episode 10000\tAverage Score: 0.22\n",
    "Elapsed : 1:50:51.814972\n",
    "2018-10-20 21:06:46.727933\n",
    "** max_t = 1000, FC32-FC32-FC2\n",
    "printing step and score stats every 500\n",
    "with max_t = 1000, larger scores, also the replay buffer is full of good actions\n",
    "10/20 10:13 PM - This is going to be a 4 hr run ! took 6 hrs. But done well.\n",
    "Episode 500\tAverage Score: 0.00\tScore Stats: [0. 0. 0. 0.]\tSteps Stats : [13. 13. 13. 14.]\n",
    "Episode 1000\tAverage Score: 0.00\tScore Stats: [0. 0. 0. 0.]\tSteps Stats : [13. 13. 13. 14.]\n",
    "Episode 1500\tAverage Score: 0.06\tScore Stats: [0.         0.         0.1        0.50000001]\tSteps Stats : [ 13.  16.  33. 112.]\n",
    "Episode 2000\tAverage Score: 0.00\tScore Stats: [0.  0.  0.  0.1]\tSteps Stats : [13. 13. 13. 31.]\n",
    "Episode 2500\tAverage Score: 0.03\tScore Stats: [0.  0.  0.1 0.1]\tSteps Stats : [16. 16. 32. 33.]\n",
    "Episode 3000\tAverage Score: 0.06\tScore Stats: [0.         0.         0.1        0.60000001]\tSteps Stats : [ 15.  17.  32. 128.]\n",
    "Episode 3500\tAverage Score: 0.06\tScore Stats: [0.         0.         0.1        0.50000001]\tSteps Stats : [ 13.   14.5  32.  111. ]\n",
    "Episode 4000\tAverage Score: 0.15\tScore Stats: [0.         0.1        0.2        0.90000001]\tSteps Stats : [ 15.75  32.    51.25 186.  ]\n",
    "Episode 4500\tAverage Score: 0.17\tScore Stats: [0.1        0.1        0.2        0.60000001]\tSteps Stats : [ 31.   45.   61.5 141. ]\n",
    "Episode 4724\tAverage Score: 0.51\tScore: 5.100\tMax_steps : 1000\tSteps Avg : 113.180\n",
    "Environment solved in 4724 episodes!\tAverage Score:  0.51\n",
    "Episode 5000\tAverage Score: 1.17\tScore Stats: [0.         0.2        1.72500003 5.20000008]\tSteps Stats : [  16.   51.  338. 1000.]\n",
    "Episode 5500\tAverage Score: 0.30\tScore Stats: [0.         0.2        0.40000001 2.60000004]\tSteps Stats : [ 14.   53.   99.5 561. ]\n",
    "Episode 6000\tAverage Score: 0.21\tScore Stats: [0.         0.1        0.3        1.20000002]\tSteps Stats : [ 13.75  45.5   82.5  303.  ]\n",
    "Episode 6500\tAverage Score: 2.99\tScore Stats: [0.3        4.00000006 5.10000008 5.20000008]\tSteps Stats : [  75.   790.5 1000.  1000. ]\n",
    "Episode 7000\tAverage Score: 2.76\tScore Stats: [0.1        3.10000005 5.20000008 5.30000008]\tSteps Stats : [  33.   596.5 1000.  1000. ]\n",
    "Episode 7500\tAverage Score: 0.70\tScore Stats: [0.         0.1        0.1        5.30000008]\tSteps Stats : [  16.     31.     33.75 1000.  ]\n",
    "Episode 8000\tAverage Score: 1.82\tScore Stats: [0.1        0.35000001 5.10000008 5.20000008]\tSteps Stats : [  32.   80. 1000. 1000.]\n",
    "Episode 8500\tAverage Score: 1.95\tScore Stats: [0.1        1.00000001 4.60000007 5.20000008]\tSteps Stats : [  32.  210. 1000. 1000.]\n",
    "Episode 9000\tAverage Score: 0.85\tScore Stats: [0.         0.         0.25       5.20000008]\tSteps Stats : [  13.   13.   64. 1000.]\n",
    "Episode 9500\tAverage Score: 2.31\tScore Stats: [0.2        1.15000002 5.10000008 5.20000008]\tSteps Stats : [  52.   237.5 1000.  1000. ]\n",
    "Episode 10000\tAverage Score: 2.72\tScore Stats: [0.3        2.05000003 5.20000008 5.30000008]\tSteps Stats : [  70.  406. 1000. 1000.]\n",
    "Elapsed : 6:00:50.424233\n",
    "2018-10-21 03:39:40.714783\n",
    "env terminates at 1000\n",
    "Questions:\n",
    "1. Separate replay buffer ?\n",
    "2.  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the state dict, because we have access to the code.\n",
    "\n",
    "Another way to save and load model, to be used by 2 distinct and separate entities is to :\n",
    "- `torch.save(model, filepath)`; \n",
    "- Then later, `model = torch.load(filepath)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(50): # 10 episodes\n",
    "    actions = np.zeros([num_agents, action_size])\n",
    "    env_info = env.reset(train_mode=True)[brain_name]  # reset the environment train_mode = false is very slow\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        actions=[]\n",
    "        for a in range(num_agents):\n",
    "            action = agent.act(states[a])\n",
    "            actions.append(action)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        score += np.max(rewards)# update the score\n",
    "        if any(dones): # exit loop if episode finished\n",
    "            break \n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
